{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gesture_framework\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Framework Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = gesture_framework.gesture_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gestures/dataset.json', 'r') as f:\n",
    "    originals = np.asarray(json.load(f))\n",
    "with open('gestures/dataset_np.json', 'r') as f:\n",
    "    originals_np = json.load(f)\n",
    "    for i in range(len(originals_np)):\n",
    "        o = (np.asarray(originals_np[i])+1)/2\n",
    "        originals_np[i] = [o.tolist()]\n",
    "with open('gestures/gestures_good.json', 'r') as f:\n",
    "    synth_data1 = np.asarray(json.load(f))\n",
    "with open('gestures/gestures_good_np.json', 'r') as f:\n",
    "    synth_data1_np = json.load(f)\n",
    "    for i in range(len(synth_data1_np)):\n",
    "        synth_data1_np[i] = [synth_data1_np[i]]\n",
    "with open('gestures/gestures_mid.json', 'r') as f:\n",
    "    synth_data2 = np.asarray(json.load(f))\n",
    "with open('gestures/gestures_mid_np.json', 'r') as f:\n",
    "    synth_data2_np = json.load(f)\n",
    "    for i in range(len(synth_data2_np)):\n",
    "        synth_data2_np[i] = [synth_data2_np[i]]\n",
    "with open('gestures/gestures_bad.json', 'r') as f:\n",
    "    synth_data3 = np.asarray(json.load(f))\n",
    "with open('gestures/gestures_bad_np.json', 'r') as f:\n",
    "    synth_data3_np = json.load(f)\n",
    "    for i in range(len(synth_data3_np)):\n",
    "        synth_data3_np[i] = [synth_data3_np[i]]\n",
    "with open('gestures/labels.json', 'r') as f:\n",
    "    labels = np.asarray(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marvi\\anaconda3\\envs\\no_tfGAN_GANs\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "classifier_tstr_empty = classifier_dau_empty = 'classifier/classifier_empty.h5'\n",
    "classifier_tstr_trained = tf.keras.models.load_model('classifier/device_classifier_7-19_new_new.h5')\n",
    "classifier_trts = tf.keras.models.load_model('classifier/device_classifier_7-19_new_new.h5')\n",
    "\n",
    "classifier_td_long = tf.keras.models.load_model('classifier/td_long.h5', compile=False)\n",
    "classifier_td_long.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "shape_long = [3,3]\n",
    "classifier_td_mid = tf.keras.models.load_model('classifier/td_mid.h5', compile=False)\n",
    "classifier_td_mid.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "shape_mid = [3,2]\n",
    "classifier_td_short = tf.keras.models.load_model('classifier/td_small.h5', compile=False)\n",
    "classifier_td_short.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "shape_short = [2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representativeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_number = 50\n",
    "kl_d1 = gf.compare_physical_properties(originals_np, synth_data1_np, bin_number)\n",
    "kl_d2 = gf.compare_physical_properties(originals_np, synth_data2_np, bin_number)\n",
    "kl_d3 = gf.compare_physical_properties(originals_np, synth_data3_np, bin_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd1 = gf.get_wasserstein_distance(originals_np[:250], synth_data1_np[:250])\n",
    "wd2 = gf.get_wasserstein_distance(originals_np[:250], synth_data2_np[:250])\n",
    "wd3 = gf.get_wasserstein_distance(originals_np[:250], synth_data3_np[:250])\n",
    "print(wd1, wd2, wd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "c = copy.copy(classifier_tstr_empty)\n",
    "tstr1 = gf.tstr(originals, synth_data1, labels, c, classifier_tstr_trained, epochs)\n",
    "tstr2 = gf.tstr(originals, synth_data1, labels, c, classifier_tstr_trained, epochs)\n",
    "tstr3 = gf.tstr(originals, synth_data1, labels, c, classifier_tstr_trained, epochs)\n",
    "print(tstr1, tstr2, tstr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty1 = gf.nnad_novelty(synth_data1_np[:250], originals_np[:250])\n",
    "novelty2 = gf.nnad_novelty(synth_data2_np[:250], originals_np[:250])\n",
    "novelty3 = gf.nnad_novelty(synth_data3_np[:250], originals_np[:250])\n",
    "print(novelty1, novelty2, novelty3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trts1 = gf.trts(synth_data1, labels, classifier_trts)\n",
    "trts2 = gf.trts(synth_data2, labels, classifier_trts)\n",
    "trts3 = gf.trts(synth_data3, labels, classifier_trts)\n",
    "print(trts1, trts2, trts3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diversity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity1 = gf.nnad_diversity(synth_data1_np[:250])\n",
    "diversity2 = gf.nnad_diversity(synth_data2_np[:250])\n",
    "diversity3 = gf.nnad_diversity(synth_data3_np[:250])\n",
    "print(diversity1, diversity2, diversity3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portions = [0, 0.1, 0.3, 0.5]\n",
    "epochs = 15\n",
    "dau1 = gf.dau(originals, synth_data1, labels, classifier_dau_empty, portions, epochs)\n",
    "dau2 = gf.dau(originals, synth_data2, labels, classifier_dau_empty, portions, epochs)\n",
    "dau3 = gf.dau(originals, synth_data3, labels, classifier_dau_empty, portions, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dau1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dau2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dau3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coherence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [classifier_td_long, classifier_td_mid, classifier_td_short]\n",
    "shapes = [shape_long, shape_mid, shape_short]\n",
    "s1 = []\n",
    "for i in range(len(synth_data1_np)):\n",
    "    s1.append(synth_data1_np[i][0])\n",
    "td1 = gf.temporal_depencies(s1, classifiers, shapes, 7)\n",
    "s2 = []\n",
    "for i in range(len(synth_data2_np)):\n",
    "    s2.append(synth_data2_np[i][0])\n",
    "td2 = gf.temporal_depencies(s2, classifiers, shapes, 7)\n",
    "s3 = []\n",
    "for i in range(len(synth_data3_np)):\n",
    "    s3.append(synth_data3_np[i][0])\n",
    "td3 = gf.temporal_depencies(s3, classifiers, shapes, 7)\n",
    "print(td1, td2, td3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_GANs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
